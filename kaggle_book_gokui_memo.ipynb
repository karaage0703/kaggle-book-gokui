{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMkKABZR51jbJ0EvQ2BIQpM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/karaage0703/kaggle-book-gokui/blob/main/kaggle_book_gokui_memo.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Kaggleに挑む深層学習プログラミングの極意\n",
        "\n",
        "読書メモです。公式のサポートサイトは以下。\n",
        "\n",
        "https://github.com/smly/kaggle-book-gokui"
      ],
      "metadata": {
        "id": "WgisZw4or0HH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第1章 機械学習コンテストの基礎知識\n",
        "\n",
        "省略"
      ],
      "metadata": {
        "id": "qfc5Z0VBr7lv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第2章 探索的データ分析とモデルの作成・検証・性能向上\n",
        "\n",
        "### 2.1 探索的データ分析\n",
        "\n",
        "ノーフリーランチ定理：あらゆる問題で性能が高い万能なモデルは存在しない\n",
        "\n",
        "メモ：基盤モデルによって、ちょっと状況変わってきた気がしなくもない"
      ],
      "metadata": {
        "id": "HZZ-sIousF2x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.2 モデルの作成\n",
        "#### 2.2.1 ニューラルネットワーク"
      ],
      "metadata": {
        "id": "4Lw7vziUxsCW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "手書き文字認識データセット読み込み"
      ],
      "metadata": {
        "id": "EiAeWz8azjUZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "id": "apBG9TErsFMU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データの中身を確認\n",
        "\n",
        "https://chusotsu-program.com/matplotlib-digits-plot/\n",
        "\n",
        "https://karaage.hatenadiary.jp/entry/2018/12/17/073000"
      ],
      "metadata": {
        "id": "eSon8uqdziiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "digits.data[0]"
      ],
      "metadata": {
        "id": "twOdj8P9zqqu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "digits.images[0]"
      ],
      "metadata": {
        "id": "_wUqo-Rwr4nB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib import pyplot as plt \n",
        "fig = plt.figure(figsize=(1, 1))\n",
        "plt.imshow(digits.images[0], cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fH1OKaBvyHz_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_and_labels = list(zip(digits.images, digits.target))\n",
        "\n",
        "for index, (image, label) in enumerate(image_and_labels[:30]):\n",
        "    plt.subplot(6, 5, index + 1)\n",
        "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
        "    plt.axis('off')\n",
        "    plt.subplots_adjust(wspace=1, hspace=1)\n",
        "    plt.title('Training: %i' % label)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "vVN3lLSmyV4W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "データ変換"
      ],
      "metadata": {
        "id": "_NywiwhZ5txg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.int64)"
      ],
      "metadata": {
        "id": "5bbx7SH40CRa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "モデル作成\n",
        "\n",
        "OptimizerをSGD->Adamに変更すると性能が上がったりします\n",
        "\n",
        "関連：サポートサイトの[issuesで質問中](https://github.com/smly/kaggle-book-gokui/issues/6)\n",
        "\n",
        "参考：[【決定版】スーパーわかりやすい最適化アルゴリズム -損失関数からAdamとニュートン法-](https://qiita.com/omiita/items/1735c1d048fe5f611f80)"
      ],
      "metadata": {
        "id": "UCOyT9J95q91"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn, optim\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(64, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 10),\n",
        ")\n",
        "model.train()\n",
        "lossfun = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "#optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
        "#optimizer = optim.SGD(model.parameters(), lr=0.02)"
      ],
      "metadata": {
        "id": "ctZFVCRz5I-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "losses = []\n",
        "\n",
        "for ep in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    out = model(X)\n",
        "\n",
        "    loss = lossfun(out, y)\n",
        "    loss.backward()\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "    losses.append(loss.item())"
      ],
      "metadata": {
        "id": "jeqAOUcR5g3f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "_, pred = torch.max(out, 1)\n",
        "print((pred == y).sum().item() / len(y))"
      ],
      "metadata": {
        "id": "Es-f-7Ry6FKU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')"
      ],
      "metadata": {
        "id": "sxt4Rljt6OZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ミニバッチ\n",
        "\n",
        "`batch_size=64`のときは、`X.shape=1797`なので、1エポックの中で`1797/64 = 28`回、学習（パラメータ更新）を繰り返すことになる。\n",
        "\n",
        "`batch_size=1797`にすると、ミニバッチ処理無しと同じ状況になる。"
      ],
      "metadata": {
        "id": "M4aI2K1-A_sP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.datasets import load_digits\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "X = torch.tensor(X, dtype=torch.float32)\n",
        "y = torch.tensor(y, dtype=torch.int64)\n",
        "\n",
        "dataset = TensorDataset(X, y)\n",
        "train_loader = DataLoader(dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "Xoj0W0Sc8k1p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = nn.Sequential(\n",
        "    nn.Linear(64, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 10),\n",
        ")\n",
        "\n",
        "model.train()\n",
        "lossfun = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)"
      ],
      "metadata": {
        "id": "GtsHi8VlRzua"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_1epoch(model, train_loader, lossfun, optimizer, device):\n",
        "    model.train()\n",
        "    total_loss, total_acc = 0.0, 0.0\n",
        "\n",
        "    for x, y in tqdm(train_loader):\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        out = model(x)\n",
        "\n",
        "        loss = lossfun(out, y)\n",
        "        loss.backward()\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item() * x.size(0)\n",
        "\n",
        "        _, pred = torch.max(out, 1)\n",
        "        total_acc += torch.sum(pred == y.data)\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader.dataset)\n",
        "    avg_acc = total_acc / len(train_loader.dataset)\n",
        "    return avg_acc, avg_loss"
      ],
      "metadata": {
        "id": "JFUhGWmwBfZw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm\n",
        "losses = []\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "#device = \"cpu\"\n",
        "\n",
        "for ep in range(100):\n",
        "    optimizer.zero_grad()\n",
        "    avg_acc, avg_loss = train_1epoch(\n",
        "        model, train_loader, lossfun, optimizer, device\n",
        "    )\n",
        "    losses.append(avg_loss)"
      ],
      "metadata": {
        "id": "I6WSiymDCK2k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(losses)\n",
        "plt.xlabel('epoch')\n",
        "plt.ylabel('loss')"
      ],
      "metadata": {
        "id": "hfQCVrYXD5J5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7EpFyFzaCoSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.2 ニューラルネットワーク以外のアルゴリズム"
      ],
      "metadata": {
        "id": "NowXAgBWFGJG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 勾配ブースティング決定木(GBDT)\n",
        "- k近傍法(kNN)\n",
        "- 線形モデル"
      ],
      "metadata": {
        "id": "Om9tX_IBKdNb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.3 利用するモデルと得意とするデータセットの種類・性質"
      ],
      "metadata": {
        "id": "1UAMaA6nLL4G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.4 ニューラルネットワークの高速化・安定化\n",
        "\n",
        "[PERFORMANCE TUNING GUIDE](https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html)"
      ],
      "metadata": {
        "id": "u14ZiM_KcvL2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.2.5 ニューラルネットワークの実装における留意点"
      ],
      "metadata": {
        "id": "55Xvoc9ZdIfo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.3 モデルの検証"
      ],
      "metadata": {
        "id": "vuUY0PVRdxYB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.1 汎化性能\n",
        "\n",
        "https://github.com/smly/kaggle-book-gokui/blob/main/chapter2/02_mlp_hold_out.py"
      ],
      "metadata": {
        "id": "o2CTUwSOd1m3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "X_train, X_valid, y_train, y_valid = train_test_split(\n",
        "    X_train, y_train, test_size=0.2, stratify=y_train, random_state=0\n",
        ")\n",
        "\n",
        "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_valid = torch.tensor(X_valid, dtype=torch.float32)\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train = torch.tensor(y_train, dtype=torch.int64)\n",
        "y_valid = torch.tensor(y_valid, dtype=torch.int64)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "val_dataset = TensorDataset(X_valid, y_valid)\n",
        "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "l-QpR8hsFIZ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from tqdm import tqdm\n",
        "\n",
        "def validate_1epoch(model, val_loader, lossfun, device):\n",
        "    model.eval()\n",
        "    total_loss, total_acc = 0.0, 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for x, y in tqdm(val_loader):\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "\n",
        "            out = model(x)\n",
        "            loss = lossfun(out, y)\n",
        "            _, pred = torch.max(out, 1)\n",
        "\n",
        "            total_loss += loss.item() * x.size(0)\n",
        "            total_acc += torch.sum(pred == y.data)\n",
        "\n",
        "    avg_loss = total_loss / len(val_loader.dataset)\n",
        "    avg_acc = total_acc / len(val_loader.dataset)\n",
        "    return avg_acc, avg_loss\n",
        "\n",
        "\n",
        "def predict(model, loader, device):\n",
        "    pred_fun = torch.nn.Softmax(dim=1)\n",
        "    preds = []\n",
        "    for x, _ in tqdm(loader):\n",
        "        with torch.set_grad_enabled(False):\n",
        "            x = x.to(device)\n",
        "            y = pred_fun(model(x))\n",
        "        y = y.cpu().numpy()\n",
        "        y = np.argmax(y, axis=1)\n",
        "        preds.append(y)\n",
        "    preds = np.concatenate(preds)\n",
        "    return preds"
      ],
      "metadata": {
        "id": "rFqjD69gemsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習・検証"
      ],
      "metadata": {
        "id": "eIr1nXqefWP-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "valid_losses = []\n",
        "\n",
        "for ep in range(100):\n",
        "    train_acc, train_loss = train_1epoch(\n",
        "        model, train_loader, lossfun, optimizer, device\n",
        "    )\n",
        "    valid_acc, valid_loss = validate_1epoch(\n",
        "        model, val_loader, lossfun, device\n",
        "    )\n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    valid_losses.append(valid_loss)"
      ],
      "metadata": {
        "id": "DjkzhDrWfRFA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(train_losses, label=\"train_losses\")\n",
        "plt.plot(valid_losses, label=\"valid_losses\")\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"loss\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "GAN3nyvKfAld"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.2 検証セットが必要な理由\n",
        "\n",
        "ホールドアウト検証(holdout validation)：訓練セット(train)から検証セット(validation)を切り出し、自分のモデルの性能を検証する"
      ],
      "metadata": {
        "id": "W7Qn9ewmfhTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.3 交差検証\n",
        "\n",
        "データ分割を異なる方法で複数回実施して、それぞれホールドアウト検証をすること。\n",
        "\n",
        "訓練セットを分割した最小単位を「fold」と呼ぶ"
      ],
      "metadata": {
        "id": "5B8KNU-ugCNQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import KFold, train_test_split\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "digits = load_digits()\n",
        "X = digits.data\n",
        "y = digits.target\n",
        "\n",
        "# train と test に分割\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test = torch.tensor(y_test, dtype=torch.int64)\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "model = nn.Sequential(\n",
        "    nn.Linear(64, 32),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(32, 16),\n",
        "    nn.ReLU(),\n",
        "    nn.Linear(16, 10),\n",
        ")\n",
        "model = model.to(device)\n",
        "\n",
        "lossfun = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
        "\n",
        "cv = KFold(n_splits=5)\n",
        "oof_train = np.zeros((len(X_train), 10))\n",
        "train_losses = []\n",
        "valid_losses = []\n",
        "train_accs = []\n",
        "valid_accs = []\n",
        "test_preds = []\n",
        "\n",
        "for fold_id, (train_index, valid_index) in enumerate(cv.split(X_train)):\n",
        "    X_tr, X_val = X_train[train_index], X_train[valid_index]\n",
        "    y_tr, y_val = y_train[train_index], y_train[valid_index]\n",
        "\n",
        "    X_tr = torch.tensor(X_tr, dtype=torch.float32)\n",
        "    X_val = torch.tensor(X_val, dtype=torch.float32)\n",
        "    y_tr = torch.tensor(y_tr, dtype=torch.int64)\n",
        "    y_val = torch.tensor(y_val, dtype=torch.int64)\n",
        "\n",
        "    train_dataset = TensorDataset(X_tr, y_tr)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=64, shuffle=False)\n",
        "    val_dataset = TensorDataset(X_val, y_val)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)"
      ],
      "metadata": {
        "id": "lMlfs77RfPzv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.3.4 多様な検証方法の使い分け\n",
        "\n",
        "stratified k-fold：各foldに含まれる正解ラベルの割合を等しく分割する\n",
        "\n",
        "group k-fold: データセット内にグループが存在する場合の分割の工夫\n",
        "\n",
        "stratified group k-fold\n",
        "\n",
        "multilabel stratified k-fold\n",
        "\n",
        "time series split\n",
        "\n",
        "adversarial validation"
      ],
      "metadata": {
        "id": "yioSoTbPiagF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2.4 性能の向上"
      ],
      "metadata": {
        "id": "s5W7fjefjiOb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.1 モデルの複雑性（表現力）"
      ],
      "metadata": {
        "id": "ZLY5lIfijniQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2.4.2 データの拡張\n",
        "\n",
        "- random erasing\n",
        "- mixup\n",
        "- 半教師あり学習\n",
        "\n",
        "https://qiita.com/karaage0703/items/07157a0406c757ef30b8\n",
        "\n",
        "https://qiita.com/karaage0703/items/9b56a68a5dba4ad330d9"
      ],
      "metadata": {
        "id": "FiICVHVgjshU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 第3章 画像分類入門\n",
        "\n",
        "### 3.1 畳み込みニューラルネットワークの基礎"
      ],
      "metadata": {
        "id": "UTBg0uf5keHx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.2 コンテスト「Dogs vs. Cats Redux」\n",
        "\n"
      ],
      "metadata": {
        "id": "dZfGAG-H5Gwx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.3 最初の学習：CNNアーキテクチャ"
      ],
      "metadata": {
        "id": "3OZ1X2D65SWs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.4 最初の学習：データセットの準備と学習ループ\n",
        "\n"
      ],
      "metadata": {
        "id": "DJ6tSChV5hoB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4.1 データセットの準備\n",
        "以下よりコード引用\n",
        "\n",
        "https://github.com/smly/kaggle-book-gokui/blob/main/chapter3/train.py\n",
        "\n",
        "引数の`dryrun = True`のときは画像の枚数を１００枚に制限することで、短時間で検証する工夫をしている。"
      ],
      "metadata": {
        "id": "hnOU00rR6UAx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def setup_train_val_split(labels, dryrun=False, seed=0):\n",
        "    x = np.arange(len(labels))\n",
        "    y = np.array(labels)\n",
        "    splitter = sklearn.model_selection.StratifiedShuffleSplit(\n",
        "        n_splits=1, train_size=0.8, random_state=seed\n",
        "    )\n",
        "    train_indices, val_indices = next(splitter.split(x, y))\n",
        "\n",
        "    if dryrun:\n",
        "        train_indices = np.random.choice(train_indices, 100, replace=False)\n",
        "        val_indices = np.random.choice(val_indices, 100, replace=False)\n",
        "\n",
        "    return train_indices, val_indices"
      ],
      "metadata": {
        "id": "eLBt5zFkhf7J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.4.2 学習ループ\n",
        "ループに`tqdm`を使って学習時間の予想を表示できるようにする"
      ],
      "metadata": {
        "id": "tPs0EqEGlnA0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.5 最適化アルゴリズムと学習率スケジューリング\n"
      ],
      "metadata": {
        "id": "YW8lQBz2lkKj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.5.1 最適化アルゴリズム\n",
        "\n",
        "「とりあえず Momentum SGDとAdamのどちらかを使う」\n",
        "\n",
        "- Adam: 早く収束してほしい\n",
        "- Momentum SGD: 汎化性能を高めたい（学習時間がかかる）\n",
        "\n",
        "※ 上記に反論するような論文もある"
      ],
      "metadata": {
        "id": "WL4nmzG3l8dO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.5.2 学習率・学習期間\n",
        "\n",
        "学習率は徐々に下げていくのが基本。汎化性能も上がることが多い。\n",
        "\n",
        "メモ： 学習空間を勾配法でくだっていくイメージをするとなんとなく気持ちを理解できる\n",
        "\n",
        "学習率のスケジュールとして cosine annealing がある（アニーリングだから、焼きなまし法か）。"
      ],
      "metadata": {
        "id": "FeCR2GpYmaqA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.6 データ拡張"
      ],
      "metadata": {
        "id": "IXdoQmEanQaW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.6.1 random flipとrandom crop"
      ],
      "metadata": {
        "id": "hNQ-SUyhnULw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.6.2 mixup\n",
        "\n",
        "mixupのような強力なデータ拡張は、学習の初期で利用して途中から利用しないテクニックが有効なことがある。"
      ],
      "metadata": {
        "id": "9sAx96Lnnb72"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.7 アンサンブル"
      ],
      "metadata": {
        "id": "72uHurVCnuDN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.7.1 交差検証とアンサンブル\n",
        "\n",
        "- averaging: 各モデルの予測を平均すること\n",
        "- seedのみ変えたモデルを averagingしても効果がでることが多い\n",
        "- 交差検証と同時にaveragingをするというテクニックがKaggleでは広く用いられている"
      ],
      "metadata": {
        "id": "Y-DgP_UdoDlF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.7.2 test time augmentation(TTA)\n",
        "\n",
        "推論時のデータ拡張"
      ],
      "metadata": {
        "id": "IigP2XVYo5T3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3.8 さらにスコアを伸ばすために\n"
      ],
      "metadata": {
        "id": "r686wuPKpOZl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.8.1 試行錯誤の順番は？\n",
        "\n",
        "「まず各コンテスト特有の点に優先的に取り組むのがよい」\n",
        "\n",
        "テクニック的なところは、最後に追い込めるのと、むやみに最初に入力画像サイズを大きくすると、試行錯誤の回数を減らしてしまう"
      ],
      "metadata": {
        "id": "QWMVsW6AqowX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.8.2 行き詰まった際に、何をする？\n",
        "\n",
        "エラー分析とか。\n",
        "\n",
        "具体的には、GradCAMとか？？"
      ],
      "metadata": {
        "id": "PYM2Jk9pqtWq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3.8.3 論文はどこでどうやって調べる\n",
        "以下は個人的メモ\n",
        "\n",
        "[機械学習の論文を探す場所まとめ](https://zenn.dev/karaage0703/articles/46806d77983b8a)"
      ],
      "metadata": {
        "id": "4hLgrdECqfNt"
      }
    }
  ]
}